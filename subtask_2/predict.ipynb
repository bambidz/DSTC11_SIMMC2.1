{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dial_path = \"../../data/simmc2.1_dials_dstc11_devtest.json\"\n",
    "scene_path = \"../../data/public\"\n",
    "image_path = \"../../data/public_image\"\n",
    "meta_path = \"../../data\"\n",
    "output_path = \"./data/object_f1_devtest_with_predict.json\"\n",
    "\n",
    "fashion_model = models.resnext101_32x8d(pretrained=False)\n",
    "n_features = fashion_model.fc.in_features\n",
    "fashion_model.fc = nn.Linear(n_features, 251)\n",
    "\n",
    "fashion_model.load_state_dict(torch.load('fashion_devtest_baseline.bin'))\n",
    "fashion_model.eval()\n",
    "\n",
    "furniture_model = models.resnext101_32x8d(pretrained=False)\n",
    "n_features = furniture_model.fc.in_features\n",
    "furniture_model.fc = nn.Linear(n_features, 30)\n",
    "\n",
    "furniture_model.load_state_dict(torch.load('furniture_devtest_baseline.bin'))\n",
    "furniture_model.eval()\n",
    "\n",
    "with open(\"./idx2fashion_id.json\") as f:\n",
    "    idx2fashion = json.load(f)\n",
    "\n",
    "with open(\"./idx2furniture_id.json\") as f:\n",
    "    idx2furniture = json.load(f)\n",
    "\n",
    "class2prefab = {}\n",
    "\n",
    "with open(os.path.join(meta_path, \"fashion_prefab_metadata_all.json\")) as f:\n",
    "    meta_data = json.load(f)\n",
    "    \n",
    "    for prefab_i in meta_data.keys():\n",
    "        class2prefab[prefab_i.replace(\"/\", \"_\")] = prefab_i\n",
    "\n",
    "with open(os.path.join(meta_path, \"furniture_prefab_metadata_all.json\")) as f:\n",
    "    meta_data = json.load(f)\n",
    "    \n",
    "    for prefab_i in meta_data.keys():\n",
    "        class2prefab[prefab_i.replace(\"/\", \"_\")] = prefab_i\n",
    "\n",
    "with open(dial_path) as f:\n",
    "    devtest_dial = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scene2object(scenes, domain):\n",
    "    if domain == \"fashion\":\n",
    "        meta_file = os.path.join(meta_path, \"fashion_prefab_metadata_all.json\")\n",
    "        model = fashion_model\n",
    "        \n",
    "        mean_nums = [0.274, 0.258, 0.259] # [0.485, 0.456, 0.406] -> [0.274, 0.258, 0.259]\n",
    "        std_nums = [0.207, 0.200, 0.204] # [0.229, 0.224, 0.225] -> [0.207, 0.200, 0.204]\n",
    "        \n",
    "        id2class = idx2fashion\n",
    "        \n",
    "        vis_meta = [\"assetType\", \"color\", \"pattern\"]\n",
    "\n",
    "    else:\n",
    "        meta_file = os.path.join(meta_path, \"furniture_prefab_metadata_all.json\")\n",
    "        model = furniture_model\n",
    "        \n",
    "        mean_nums = [0.504, 0.470, 0.438] # [0.485, 0.456, 0.406] -> [0.504, 0.470, 0.438]\n",
    "        std_nums = [0.297, 0.287, 0.274] # [0.229, 0.224, 0.225] -> [0.297, 0.287, 0.274]\n",
    "\n",
    "        id2class = idx2furniture\n",
    "\n",
    "        vis_meta = [\"color\", \"type\"]\n",
    "\n",
    "    model.cuda()\n",
    "\n",
    "    test_transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Resize(size=[256,256]),\n",
    "        T.CenterCrop(size=256),\n",
    "        T.Normalize(mean_nums, std_nums)\n",
    "    ])\n",
    "        \n",
    "    scene_ids = scenes.items()\n",
    "    scene_objects_data = {}\n",
    "    \n",
    "    for scene_num, scene_id in scene_ids:\n",
    "        scene_i_path = os.path.join(scene_path, scene_id + \"_scene.json\")\n",
    "        \n",
    "        image_i_id = scene_id.replace(\"m_cloth\", \"cloth\")\n",
    "        image_i_path = os.path.join(image_path, image_i_id + \".png\")\n",
    "        \n",
    "        with open(scene_i_path) as f:\n",
    "            scene_i_data = json.load(f)\n",
    "        \n",
    "        for scene_info in scene_i_data[\"scenes\"]:\n",
    "            # print(scene_data_i)\n",
    "            # scene_info = scene_data_i[0]\n",
    "\n",
    "            objects_i = scene_info['objects']\n",
    "            # relationships_i = scene_info['relationships']\n",
    "            \n",
    "            for object_i in objects_i:\n",
    "                try:\n",
    "                    scene_objects_data[object_i['index']].append({'prefab_path': object_i['prefab_path'], 'bbox': object_i['bbox'],\n",
    "                                                                  'position': object_i['position'], 'image_path': image_i_path, 'turn': scene_num})\n",
    "                except:\n",
    "                    scene_objects_data[object_i['index']] = [{'prefab_path': object_i['prefab_path'], 'bbox': object_i['bbox'],\n",
    "                                                              'position': object_i['position'], 'image_path': image_i_path, 'turn': scene_num}]\n",
    "\n",
    "\n",
    "    objects_meta = {}\n",
    "\n",
    "    with open(meta_file) as f:\n",
    "        meta_data = json.load(f)\n",
    "    \n",
    "    for object_index in scene_objects_data.keys():\n",
    "        total_logit = None\n",
    "        min_turn = 9999\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for object_index_info in scene_objects_data[object_index]:\n",
    "                scene_img = Image.open(object_index_info['image_path'])\n",
    "                bbox = object_index_info['bbox']\n",
    "                turn = int(object_index_info['turn'])\n",
    "                \n",
    "                if int(turn) < min_turn:\n",
    "                    min_turn = turn\n",
    "                \n",
    "                if bbox[2] < 2:\n",
    "                    bbox[1] = max(0, bbox[1] - 2)\n",
    "                    bbox[2] = 4\n",
    "                \n",
    "                if bbox[3] < 2:\n",
    "                    bbox[0] = max(0, bbox[0] - 2)\n",
    "                    bbox[3] = 4\n",
    "                \n",
    "                # object에 해당하는 부분만 crop 및 tensor로 변환\n",
    "\n",
    "                object_img = scene_img.crop((bbox[0], bbox[1], bbox[0] + bbox[3], bbox[1] + bbox[2]))\n",
    "                image = np.array(object_img.convert('RGB'))\n",
    "                rgb_img = np.float32(image) / 255\n",
    "\n",
    "                input_tensor = test_transform(rgb_img).unsqueeze(0)\n",
    "                input_tensor = input_tensor.cuda()\n",
    "                \n",
    "                output_logit = model(input_tensor)\n",
    "                \n",
    "                if total_logit is None:\n",
    "                    total_logit = output_logit\n",
    "                else:\n",
    "                    total_logit += output_logit\n",
    "\n",
    "            total_logit = total_logit[0].data\n",
    "            # print(torch.argmax(total_logit).item(), bbox)\n",
    "            \n",
    "        pred_prefab = class2prefab[id2class[str(torch.argmax(total_logit).item())]]\n",
    "            \n",
    "        pred_meta = meta_data[pred_prefab]\n",
    "        \n",
    "        ans_prefab = scene_objects_data[object_index][0]['prefab_path']\n",
    "        ans_meta = meta_data[ans_prefab]\n",
    "        \n",
    "        # logit으로 prefab 예측\n",
    "        # 정답 prefab로부터 meta data 가져오기\n",
    "            \n",
    "        final_meta_data = {k:v for k,v in ans_meta.items()}\n",
    "        final_meta_data.update({k:pred_meta[k] for k in vis_meta})\n",
    "        \n",
    "        objects_meta[object_index] = final_meta_data\n",
    "        objects_meta[object_index].update({\"turn\":min_turn})\n",
    "    \n",
    "    model.cpu()\n",
    "        \n",
    "    return objects_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dial2data(data_i):\n",
    "    dial_i = data_i['dialogue']\n",
    "    domain = data_i['domain']\n",
    "    scenes = data_i['scene_ids']\n",
    "    \n",
    "    meta_dict = scene2object(scenes, domain)\n",
    "\n",
    "    transcript_list = []\n",
    "    system_list = []\n",
    "    label_list = []\n",
    "    mentioned_object_list = []\n",
    "\n",
    "    prev_metioned_object = set()\n",
    "    for turn_i in dial_i:\n",
    "        mentioned_object_list.append(list(prev_metioned_object))\n",
    "\n",
    "        transcript_list.append(turn_i['transcript'])\n",
    "        system_list.append(turn_i['system_transcript'])\n",
    "        label_list.append(turn_i['transcript_annotated']['act_attributes']['objects'])\n",
    "        # prev_metioned_object.update(turn_i['transcript_annotated']['act_attributes']['objects'])\n",
    "        prev_metioned_object.update(turn_i['system_transcript_annotated']['act_attributes']['objects'])\n",
    "    \n",
    "    return transcript_list, system_list, label_list, meta_dict, mentioned_object_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1687/1687 [43:31<00:00,  1.55s/it] \n"
     ]
    }
   ],
   "source": [
    "total_data = []\n",
    "\n",
    "for idx in tqdm(range(len(devtest_dial[\"dialogue_data\"]))):\n",
    "    t_list, s_list, l_list, m_dict, o_list = dial2data(devtest_dial[\"dialogue_data\"][idx])\n",
    "    \n",
    "    total_data.append({'transcript': t_list, 'system_transcript': s_list, 'labels': l_list, 'meta': m_dict, 'mentioned_object' : o_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcript': ['Do you have any plain jeans?', 'Sorry, I misspoke. Can you show me dresses instead?', 'Does the grey have good reviews?', 'The grey one on the hanging rack.', 'I was kind of looking for something more floral.', 'How about something with a pattern that will make me look taller? A blouse from New Fashion maybe?', 'Do you have any good shirts at all?', 'That brown one should work for me.', 'What do you have from The Vegan Baker?', 'What can you tell me about that black dress? How much is it? Do you have my size?'], 'system_transcript': ['What do you think of the grey pair on the left?', \"There's a maroon one on the wall on the right, and a brown one and a grey one on the rack.\", 'Which one do you mean?', 'That dress has a high rating at 4.3.', \"I'm sorry. I don't have any floral dresses right now.\", \"I'm sorry. I don't have any blouses like that from New Fashion.\", 'I have a grey one and a brown one on the table over there.', \"Okay. I'll add it to the cart for you.\", \"I don't have anything from that brand right now.\", 'Which one do you mean?'], 'labels': [[], [], [36], [36], [], [], [], [26], [], [23]], 'meta': {0: {'assetType': 'blouse_hanging', 'customerReview': 3.6, 'availableSizes': ['S', 'XS', 'L', 'XL'], 'color': 'red, white', 'pattern': 'plaid', 'brand': 'The Vegan Baker', 'sleeveLength': 'long', 'type': 'blouse', 'price': 29.99, 'size': 'L'}, 1: {'assetType': 'blouse_hanging', 'customerReview': 4.5, 'availableSizes': ['XXL', 'XS', 'XL'], 'color': 'pink, white', 'pattern': 'light spots', 'brand': 'Art Den', 'sleeveLength': 'short', 'type': 'blouse', 'price': 54.99, 'size': 'XL'}, 2: {'assetType': 'blouse_hanging', 'customerReview': 3.5, 'availableSizes': ['XXL', 'S', 'L'], 'color': 'green, violet, pink', 'pattern': 'leafy design', 'brand': 'Cats Are Great', 'sleeveLength': 'short', 'type': 'blouse', 'price': 19.99, 'size': 'XXL'}, 39: {'assetType': 'jacket_hanging', 'customerReview': 2.6, 'availableSizes': ['XXL', 'S', 'XL'], 'color': 'light grey', 'pattern': 'plain', 'brand': '212 Local', 'sleeveLength': 'full', 'type': 'jacket', 'price': 44.99, 'size': 'XL'}, 40: {'assetType': 'blouse_hanging', 'customerReview': 3.6, 'availableSizes': ['S', 'XS', 'L'], 'color': 'black, white', 'pattern': 'vertical design', 'brand': 'Pedals & Gears', 'sleeveLength': 'short', 'type': 'blouse', 'price': 19.99, 'size': 'S'}, 41: {'assetType': 'blouse_hanging', 'customerReview': 3.1, 'availableSizes': ['L', 'XL', 'M', 'XS'], 'color': 'black', 'pattern': 'knit', 'brand': 'Downtown Consignment', 'sleeveLength': 'long', 'type': 'sweater', 'price': 209.99, 'size': 'XL'}, 10: {'assetType': 'jacket_hanging', 'customerReview': 4.8, 'availableSizes': ['L', 'XXL', 'XS', 'M'], 'color': 'black', 'pattern': 'plain', 'brand': 'Cats Are Great', 'sleeveLength': 'long', 'type': 'coat', 'price': 139.99, 'size': 'L'}, 12: {'assetType': 'tshirt_hanging', 'customerReview': 4.4, 'availableSizes': ['XXL', 'XS', 'L', 'M', 'S', 'XL'], 'color': 'yellow, white', 'pattern': 'floral', 'brand': 'Cats Are Great', 'sleeveLength': 'sleeveless', 'type': 'tank top', 'price': 4.99, 'size': 'XS'}, 13: {'assetType': 'tshirt_hanging', 'customerReview': 4.9, 'availableSizes': ['XL', 'M', 'L'], 'color': 'blue', 'pattern': 'plain', 'brand': 'Downtown Stylists', 'sleeveLength': 'half', 'type': 'tshirt', 'price': 59.99, 'size': 'XL'}, 42: {'assetType': 'dress_hanging', 'customerReview': 3.2, 'availableSizes': ['XS', 'XL', 'L', 'S', 'XXL', 'M'], 'color': 'maroon', 'pattern': 'plain', 'brand': 'Nature Photographers', 'sleeveLength': 'sleeveless', 'type': 'dress', 'price': 184.99, 'size': 'L'}, 11: {'assetType': 'blouse_hanging', 'customerReview': 2.9, 'availableSizes': ['XS', 'M', 'XL', 'S'], 'color': 'white, grey', 'pattern': 'leafy design', 'brand': 'StyleNow Feed', 'sleeveLength': 'long', 'type': 'blouse', 'price': 54.99, 'size': 'S'}, 43: {'assetType': 'dress_hanging', 'customerReview': 4.2, 'availableSizes': ['L', 'XXL', 'XS', 'S', 'M', 'XL'], 'color': 'brown', 'pattern': 'plain', 'brand': 'Pedals & Gears', 'sleeveLength': 'sleeveless', 'type': 'dress', 'price': 239.99, 'size': 'M'}, 44: {'assetType': 'dress_hanging', 'customerReview': 3.6, 'availableSizes': ['XL', 'M', 'XXL', 'XS', 'L'], 'color': 'purple', 'pattern': 'plain', 'brand': 'Uptown Gallery', 'sleeveLength': 'sleeveless', 'type': 'dress', 'price': 224.99, 'size': 'XL'}, 30: {'assetType': 'dress_hanging', 'customerReview': 4.2, 'availableSizes': ['M', 'XL'], 'color': 'red', 'pattern': 'plain', 'brand': 'Downtown Stylists', 'sleeveLength': 'sleeveless', 'type': 'dress', 'price': 74.99, 'size': 'XL'}, 35: {'assetType': 'dress_hanging', 'customerReview': 4.2, 'availableSizes': ['L', 'XXL', 'XS', 'S', 'M', 'XL'], 'color': 'brown', 'pattern': 'plain', 'brand': 'Pedals & Gears', 'sleeveLength': 'sleeveless', 'type': 'dress', 'price': 239.99, 'size': 'M'}, 25: {'assetType': 'dress_hanging', 'customerReview': 4.3, 'availableSizes': ['XS', 'XL', 'S', 'XXL', 'L', 'M'], 'color': 'grey', 'pattern': 'plain', 'brand': 'Yogi Fit', 'sleeveLength': 'sleeveless', 'type': 'dress', 'price': 124.99, 'size': 'XXL'}, 19: {'assetType': 'tshirt_folded', 'customerReview': 4.2, 'availableSizes': ['XL', 'XS', 'XXL', 'S'], 'color': 'blue, white', 'pattern': 'light vertical stripes', 'brand': 'Brain Puzzles', 'sleeveLength': 'long', 'type': 'shirt', 'price': 74.99, 'size': 'XL'}, 18: {'assetType': 'tshirt_folded', 'customerReview': 3.6, 'availableSizes': ['XS', 'S', 'L', 'XXL'], 'color': 'green', 'pattern': 'plain', 'brand': 'River Chateau', 'sleeveLength': 'long', 'type': 'sweater', 'price': 164.99, 'size': 'XS'}, 20: {'assetType': 'tshirt_folded', 'customerReview': 3.4, 'availableSizes': ['L', 'M'], 'color': 'maroon', 'pattern': 'plain', 'brand': 'Garden Retail', 'sleeveLength': 'long', 'type': 'sweater', 'price': 124.99, 'size': 'L'}, 24: {'assetType': 'tshirt_folded', 'customerReview': 4.9, 'availableSizes': ['L', 'XXL', 'XL', 'S', 'XS'], 'color': 'grey', 'pattern': 'plain', 'brand': 'Garden Retail', 'sleeveLength': 'long', 'type': 'shirt', 'price': 84.99, 'size': 'XXL'}, 21: {'assetType': 'tshirt_folded', 'customerReview': 4.4, 'availableSizes': ['L', 'M', 'XS'], 'color': 'grey', 'pattern': 'plain', 'brand': 'Brain Puzzles', 'sleeveLength': 'long', 'type': 'shirt', 'price': 39.99, 'size': 'XS'}, 22: {'assetType': 'tshirt_folded', 'customerReview': 4.4, 'availableSizes': ['XXL', 'L', 'XL', 'XS', 'S'], 'color': 'olive, white', 'pattern': 'vertical stripes', 'brand': 'Art Den', 'sleeveLength': 'long', 'type': 'shirt', 'price': 74.99, 'size': 'S'}, 28: {'assetType': 'tshirt_folded', 'customerReview': 3.3, 'availableSizes': ['XS', 'M'], 'color': 'maroon', 'pattern': 'plain', 'brand': 'Glam Nails', 'sleeveLength': 'long', 'type': 'sweater', 'price': 64.99, 'size': 'M'}, 31: {'assetType': 'dress_hanging', 'customerReview': 4.3, 'availableSizes': ['L', 'M', 'XXL', 'S', 'XL'], 'color': 'beige', 'pattern': 'plain', 'brand': 'Fancy Nails', 'sleeveLength': 'sleeveless', 'type': 'dress', 'price': 229.99, 'size': 'M'}, 15: {'assetType': 'tshirt_folded', 'customerReview': 3.1, 'availableSizes': ['XS', 'XXL', 'S'], 'color': 'black', 'pattern': 'plain', 'brand': 'Pedals & Gears', 'sleeveLength': 'long', 'type': 'sweater', 'price': 204.99, 'size': 'S'}, 16: {'assetType': 'tshirt_folded', 'customerReview': 3.5, 'availableSizes': ['L', 'S', 'M', 'XS'], 'color': 'white', 'pattern': 'plain', 'brand': 'The Vegan Baker', 'sleeveLength': 'long', 'type': 'shirt', 'price': 69.99, 'size': 'S'}, 17: {'assetType': 'tshirt_folded', 'customerReview': 4.4, 'availableSizes': ['XL', 'XXL'], 'color': 'white, black', 'pattern': 'plain', 'brand': 'Uptown Gallery', 'sleeveLength': 'long', 'type': 'sweater', 'price': 29.99, 'size': 'XL'}, 26: {'assetType': 'tshirt_folded', 'customerReview': 4.1, 'availableSizes': ['XL', 'XS', 'L'], 'color': 'brown', 'pattern': 'vertical stripes', 'brand': 'Art Den', 'sleeveLength': 'long', 'type': 'shirt', 'price': 89.99, 'size': 'XL'}, 27: {'assetType': 'tshirt_folded', 'customerReview': 4.5, 'availableSizes': ['XXL', 'M'], 'color': 'pink', 'pattern': 'plain', 'brand': 'Downtown Stylists', 'sleeveLength': 'long', 'type': 'sweater', 'price': 169.99, 'size': 'M'}, 14: {'assetType': 'dress_hanging', 'customerReview': 4.7, 'availableSizes': ['M'], 'color': 'dark brown', 'pattern': 'plain', 'brand': 'Global Voyager', 'sleeveLength': 'sleeveless', 'type': 'dress', 'price': 24.99, 'size': 'M'}, 23: {'assetType': 'dress_hanging', 'customerReview': 4.5, 'availableSizes': ['L'], 'color': 'black', 'pattern': 'plain', 'brand': '212 Local', 'sleeveLength': 'sleeveless', 'type': 'dress', 'price': 174.99, 'size': 'L'}, 34: {'assetType': 'dress_hanging', 'customerReview': 4.3, 'availableSizes': ['L', 'M', 'XXL', 'S', 'XL'], 'color': 'beige', 'pattern': 'plain', 'brand': 'Fancy Nails', 'sleeveLength': 'sleeveless', 'type': 'dress', 'price': 229.99, 'size': 'M'}, 36: {'assetType': 'dress_hanging', 'customerReview': 4.3, 'availableSizes': ['XS', 'XL', 'S', 'XXL', 'L', 'M'], 'color': 'grey', 'pattern': 'plain', 'brand': 'Yogi Fit', 'sleeveLength': 'sleeveless', 'type': 'dress', 'price': 124.99, 'size': 'XXL'}, 33: {'assetType': 'dress_hanging', 'customerReview': 4.1, 'availableSizes': ['XS'], 'color': 'black, olive', 'pattern': 'plain', 'brand': 'Home Store', 'sleeveLength': 'sleeveless', 'type': 'dress', 'price': 149.99, 'size': 'XS'}, 45: {'assetType': 'trousers_display', 'customerReview': 3.5, 'availableSizes': ['XS', 'S'], 'color': 'grey', 'pattern': 'heavy stripes', 'brand': 'The Vegan Baker', 'sleeveLength': '', 'type': 'trousers', 'price': 244.99, 'size': 'XS'}, 29: {'assetType': 'trousers_display', 'customerReview': 4.3, 'availableSizes': ['M', 'XS', 'XL', 'S', 'XXL', 'L'], 'color': 'grey', 'pattern': 'plain', 'brand': 'Cats Are Great', 'sleeveLength': '', 'type': 'jeans', 'price': 164.99, 'size': 'L'}, 46: {'assetType': 'jacket_hanging', 'customerReview': 4.1, 'availableSizes': ['S', 'XL'], 'color': 'brown', 'pattern': 'plain', 'brand': 'Downtown Stylists', 'sleeveLength': 'full', 'type': 'coat', 'price': 59.99, 'size': 'XL'}, 38: {'assetType': 'jacket_hanging', 'customerReview': 4.5, 'availableSizes': ['XS', 'XXL', 'XL', 'M', 'L'], 'color': 'black', 'pattern': 'plain', 'brand': 'North Lodge', 'sleeveLength': 'full', 'type': 'coat', 'price': 109.99, 'size': 'L'}, 47: {'assetType': 'jacket_hanging', 'customerReview': 4.0, 'availableSizes': ['S', 'XL', 'XXL'], 'color': 'brown', 'pattern': 'plain', 'brand': '212 Local', 'sleeveLength': 'full', 'type': 'hoodie', 'price': 144.99, 'size': 'XXL'}, 48: {'assetType': 'blouse_hanging', 'customerReview': 3.1, 'availableSizes': ['S', 'M', 'XL', 'L', 'XS'], 'color': 'white, black', 'pattern': 'spots', 'brand': 'New Fashion', 'sleeveLength': 'sleeveless', 'type': 'blouse', 'price': 19.99, 'size': 'XS'}, 49: {'assetType': 'blouse_hanging', 'customerReview': 3.9, 'availableSizes': ['M', 'L', 'XS', 'XL'], 'color': 'grey, white', 'pattern': 'spots', 'brand': 'Nature Photographers', 'sleeveLength': 'sleeveless', 'type': 'blouse', 'price': 69.99, 'size': 'L'}, 50: {'assetType': 'jacket_hanging', 'customerReview': 4.0, 'availableSizes': ['S', 'XL', 'XXL'], 'color': 'brown', 'pattern': 'plain', 'brand': '212 Local', 'sleeveLength': 'full', 'type': 'hoodie', 'price': 144.99, 'size': 'XXL'}, 51: {'assetType': 'blouse_hanging', 'customerReview': 4.8, 'availableSizes': ['S', 'XL', 'M', 'XS'], 'color': 'black, white', 'pattern': 'horizontal stripes', 'brand': 'HairDo', 'sleeveLength': 'sleeveless', 'type': 'blouse', 'price': 54.99, 'size': 'M'}, 3: {'assetType': 'blouse_hanging', 'customerReview': 3.8, 'availableSizes': ['XXL', 'S', 'XL', 'L'], 'color': 'maroon, white, blue', 'pattern': 'holiday', 'brand': 'Global Voyager', 'sleeveLength': 'long', 'type': 'blouse', 'price': 39.99, 'size': 'S'}, 4: {'assetType': 'jacket_hanging', 'customerReview': 4.6, 'availableSizes': ['L', 'XS', 'XXL'], 'color': 'grey', 'pattern': 'plain', 'brand': 'Art News Today', 'sleeveLength': 'full', 'type': 'coat', 'price': 59.99, 'size': 'XS'}, 5: {'assetType': 'jacket_hanging', 'customerReview': 4.0, 'availableSizes': ['S', 'XL'], 'color': 'black', 'pattern': 'plain', 'brand': 'Yogi Fit', 'sleeveLength': 'full', 'type': 'jacket', 'price': 174.99, 'size': 'XL'}, 6: {'assetType': 'blouse_hanging', 'customerReview': 2.9, 'availableSizes': ['M'], 'color': 'light grey', 'pattern': 'plain', 'brand': 'Brain Puzzles', 'sleeveLength': 'short', 'type': 'blouse', 'price': 39.99, 'size': 'M'}, 7: {'assetType': 'blouse_hanging', 'customerReview': 3.9, 'availableSizes': ['M', 'L', 'XS', 'XL'], 'color': 'grey, white', 'pattern': 'spots', 'brand': 'Nature Photographers', 'sleeveLength': 'sleeveless', 'type': 'blouse', 'price': 69.99, 'size': 'L'}, 8: {'assetType': 'jacket_hanging', 'customerReview': 3.2, 'availableSizes': ['XXL', 'XS', 'S', 'M'], 'color': 'brown', 'pattern': 'plain', 'brand': 'Global Voyager', 'sleeveLength': 'long', 'type': 'jacket', 'price': 199.99, 'size': 'S'}, 9: {'assetType': 'blouse_hanging', 'customerReview': 3.4, 'availableSizes': ['M', 'S', 'XS', 'XL', 'XXL', 'L'], 'color': 'white, grey', 'pattern': 'leafy design', 'brand': 'HairDo', 'sleeveLength': 'long', 'type': 'blouse', 'price': 74.99, 'size': 'XS'}, 32: {'assetType': 'trousers_display', 'customerReview': 3.0, 'availableSizes': ['XS'], 'color': 'grey', 'pattern': 'light stripes', 'brand': 'Coats & More', 'sleeveLength': '', 'type': 'trousers', 'price': 189.99, 'size': 'XS'}, 37: {'assetType': 'trousers_display', 'customerReview': 4.3, 'availableSizes': ['M', 'XS', 'XL', 'S', 'XXL', 'L'], 'color': 'grey', 'pattern': 'plain', 'brand': 'Cats Are Great', 'sleeveLength': '', 'type': 'jeans', 'price': 164.99, 'size': 'L'}}, 'mentioned_object': [[], [29], [42, 36, 29, 14], [42, 36, 29, 14], [42, 36, 29, 14], [42, 36, 29, 14], [42, 36, 29, 14], [36, 42, 14, 21, 26, 29], [36, 42, 14, 21, 26, 29], [36, 42, 14, 21, 26, 29]]}\n"
     ]
    }
   ],
   "source": [
    "print(total_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_turns = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./result/devtest_pred.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subtask2_RoBERTa(\n",
       "  (encoder): RobertaForMaskedLM(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50271, 1024)\n",
       "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 1024)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (18): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (19): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (20): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (21): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (22): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (23): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): RobertaLMHead(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (decoder): Linear(in_features=1024, out_features=50271, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, Trainer, TrainingArguments\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "\n",
    "special_tokens = {}\n",
    "additional_special_tokens = [\"<MO>\", \"<NMO>\", \"<UT>\", \"<ST>\", \"<P1>\", \"<P2>\"]\n",
    "special_tokens[\"additional_special_tokens\"] = additional_special_tokens\n",
    "\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "def softmax(x):\n",
    "    x_max = np.max(x, keepdims=True)\n",
    "    e_x = np.exp(x - x_max)\n",
    "    x_sum = np.sum(e_x, keepdims=True)\n",
    "    \n",
    "    f_x = e_x / x_sum\n",
    "\n",
    "    return f_x\n",
    "\n",
    "model_path = \"./save_model/checkpoint-28000/pytorch_model.bin\"\n",
    "\n",
    "class subtask2_RoBERTa(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(subtask2_RoBERTa, self).__init__()\n",
    "        self.encoder = RobertaForMaskedLM.from_pretrained(\"roberta-large\")\n",
    "        self.encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#    def forward(self, ids, mask, label1, label2):\n",
    "    def forward(self, ids, mask, label1):\n",
    "        encoder_output = self.encoder(\n",
    "            ids, \n",
    "            labels = label1,\n",
    "            attention_mask=mask,\n",
    "        )\n",
    "        \n",
    "        loss = encoder_output.loss\n",
    "        logit = encoder_output.logits\n",
    "        \n",
    "        labels = label1[label1!=-100]\n",
    "        logits = logit[label1!=-100]\n",
    "        \n",
    "        y_logits = logits[:, 9904].data\n",
    "        n_logits = logits[:, 3084].data\n",
    "        \n",
    "        # Yes index / \"Yes\", \" Yes\"\n",
    "        labels[labels==9904] = 1\n",
    "        \n",
    "        # No index / \"No\", \" No\"\n",
    "        labels[labels==3084] = 0\n",
    "                \n",
    "        return {'yes': y_logits, 'no': n_logits, 'label': labels, 'loss': loss}      \n",
    "\n",
    "model = subtask2_RoBERTa()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlplab11/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m masks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(mask_inputs \u001b[39m==\u001b[39m tokenizer\u001b[39m.\u001b[39mmask_token_id, mask_inputs, \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlplab11/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m:encoded_inputs[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcuda(), \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m:encoded_inputs[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcuda()}\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnlplab11/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(model_inputs[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], model_inputs[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m], masks)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlplab11/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m y_logit \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39myes\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlplab11/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m n_logit \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mno\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/kdy_cv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb Cell 10\u001b[0m in \u001b[0;36msubtask2_RoBERTa.forward\u001b[0;34m(self, ids, mask, label1)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlplab11/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, ids, mask, label1):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnlplab11/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     encoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlplab11/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m         ids, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlplab11/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m         labels \u001b[39m=\u001b[39;49m label1,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlplab11/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mmask,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlplab11/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlplab11/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     loss \u001b[39m=\u001b[39m encoder_output\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnlplab11/home/nlplab11/kdy/DSTC10_SIMMC2.0/model/task2/RoBERTa/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     logit \u001b[39m=\u001b[39m encoder_output\u001b[39m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/anaconda3/envs/kdy_cv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/kdy_cv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1095\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[39m    Used to hide legacy arguments that have been deprecated.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1095\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1096\u001b[0m     input_ids,\n\u001b[1;32m   1097\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1098\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1099\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1100\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1101\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1102\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1103\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1104\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1105\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1106\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1107\u001b[0m )\n\u001b[1;32m   1108\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1109\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdy_cv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MENTIONED_OBJECT = \"<MO>\"\n",
    "NOT_MENTIONED_OBJECT = \"<NMO>\"\n",
    "USER_TURN = \"<UT>\"\n",
    "SYSTEM_TURN = \"<ST>\"\n",
    "\n",
    "json_data = total_data\n",
    "    \n",
    "data = []\n",
    "pred_result = []\n",
    "\n",
    "for data_i in tqdm(json_data): # 발화\n",
    "    n = len(data_i['transcript'])\n",
    "    \n",
    "    for idx in range(n): # Turn\n",
    "        \n",
    "        dial_list = []\n",
    "        \n",
    "        for i in range(min(n_turns - 1, idx)):\n",
    "            dial_list.append(USER_TURN + \" \" + data_i['transcript'][idx + 1 - n_turns - i])\n",
    "            dial_list.append(SYSTEM_TURN + \" \" + data_i['system_transcript'][idx + 1 - n_turns - i])\n",
    "        \n",
    "        dial_list.append(USER_TURN + \" \" + data_i['transcript'][idx])\n",
    "        \n",
    "        dial_i = \" \".join(dial_list)\n",
    "        mentioned_list = data_i['mentioned_object'][idx]\n",
    "\n",
    "        object_list = []\n",
    "        \n",
    "        for object_id in data_i['meta'].keys():\n",
    "            object_meta = data_i['meta'][object_id]\n",
    "                \n",
    "            if int(object_id) in mentioned_list:\n",
    "                is_mentioned = MENTIONED_OBJECT\n",
    "            else:\n",
    "                is_mentioned = NOT_MENTIONED_OBJECT\n",
    "                \n",
    "            meta_keys = sorted(list(object_meta.keys()))\n",
    "            \n",
    "            if \"availableSizes\" in meta_keys:\n",
    "                meta_keys.remove(\"availableSizes\")\n",
    "\n",
    "            meta_list = [str(meta_key) + \" is \" + str(object_meta[meta_key]) + \".\" for meta_key in meta_keys]\n",
    "            meta_str = \" \".join(meta_list)\n",
    "\n",
    "            text = dial_i + \" \" + \" <P1><P2><mask> , \" + is_mentioned + \" \" + meta_str\n",
    "            encoded_inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", add_special_tokens=True)\n",
    "            mask_inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", add_special_tokens=True)[\"input_ids\"]\n",
    "            \n",
    "            masks = torch.where(mask_inputs == tokenizer.mask_token_id, mask_inputs, -100).cuda()\n",
    "            \n",
    "            model_inputs = {'input_ids':encoded_inputs['input_ids'].cuda(), 'attention_mask':encoded_inputs['attention_mask'].cuda()}\n",
    "            outputs = model(model_inputs['input_ids'], model_inputs['attention_mask'], masks)\n",
    "            \n",
    "            y_logit = outputs[\"yes\"].item()\n",
    "            n_logit = outputs[\"no\"].item()\n",
    "            \n",
    "            logit = [y_logit, n_logit]\n",
    "            probs = softmax(logit)\n",
    "            \n",
    "            if probs[0] >= 0.7:\n",
    "                object_list.append(str(object_id))\n",
    "        \n",
    "        pred_result.append(object_list)\n",
    "                \n",
    "                \n",
    "with open(output_path, \"w\") as f:\n",
    "    for pred_i in pred_result:\n",
    "        f.write(\" \".join(pred_i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ad9710f9d34bd9de9410e17c4ab3e50ed217ee6ca0e99af9f5e52e470309cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('kdy_cv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
